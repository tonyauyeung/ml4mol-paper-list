# The following customized entries should be added manually to each bibtex:
# 1. tags: {}
# 2. tdlr: {}


@misc{kaba2023equivariancelearnedcanonicalizationfunctions,
      title={Equivariance with Learned Canonicalization Functions}, 
      author={SÃ©kou-Oumar Kaba and Arnab Kumar Mondal and Yan Zhang and Yoshua Bengio and Siamak Ravanbakhsh},
      year={2023},
      eprint={2211.06489},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.06489},
      tags={equivariance, symmetry},
      tdlr={This paper introduces a canonicalization function for equipping equivariance to arbitrary functions. It is similar to Frame-Average, while constraining the frame space to have only a single function, i.e. the canonicalization function.},
}

@misc{puny2022frameaveraginginvariantequivariant,
      title={Frame Averaging for Invariant and Equivariant Network Design}, 
      author={Omri Puny and Matan Atzmon and Heli Ben-Hamu and Ishan Misra and Aditya Grover and Edward J. Smith and Yaron Lipman},
      year={2022},
      eprint={2110.03336},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.03336}, 
      tags={equivariance, symmetry},
      tdlr={This paper introduces frame-average, a way to symmeterize an arbitray function to have desired geometry property such as equivariance and invariance, where the frame is learnable and conditioned on the input.},
}

@misc{zhang2025symdiffequivariantdiffusionstochastic,
      title={SymDiff: Equivariant Diffusion via Stochastic Symmetrisation}, 
      author={Leo Zhang and Kianoosh Ashouritaklimi and Yee Whye Teh and Rob Cornish},
      year={2025},
      eprint={2410.06262},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.06262},
      tags={equivariance, symmetry},
      tdlr={},
}

@misc{joshi2025allatomdiffusiontransformersunified,
      title={All-atom Diffusion Transformers: Unified generative modelling of molecules and materials}, 
      author={Chaitanya K. Joshi and Xiang Fu and Yi-Lun Liao and Vahe Gharakhanyan and Benjamin Kurt Miller and Anuroop Sriram and Zachary W. Ulissi},
      year={2025},
      eprint={2503.03965},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.03965},
      tags={foundation model, diffusion, transformer},
      tdlr={The paper proposed an unified framework for molecules and materials generation, which employs a transformer-based VAE for latent representation learning and a DiT for latent generation. While the equivariance in this paper is achieved by data augmentation.},
}